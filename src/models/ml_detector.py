"""
Machine Learning Fraud Detection Module
Implements ML models for predictive fraud detection.
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import joblib
import os


class MLFraudDetector:
    """
    Machine learning-based fraud detection using ensemble methods.
    """
    
    def __init__(self, model_path: str = "models/fraud_detector.pkl"):
        self.model_path = model_path
        self.rf_model = None
        self.isolation_model = None
        self.scaler = StandardScaler()
        self.is_trained = False
        
        # Load existing model if available
        self._load_model()
    
    def train_model(self, features_df: pd.DataFrame, labels: Optional[List[int]] = None):
        """
        Train the fraud detection model.
        
        Args:
            features_df: DataFrame with extracted features
            labels: Binary labels (1 for fraud, 0 for legitimate)
        """
        print("ðŸ¤– Training ML Fraud Detection Model...")
        
        try:
            # Prepare features - be more flexible with available columns
            available_features = self._get_available_features(features_df)
            if len(available_features) == 0:
                print("âš ï¸ No valid features found for training")
                return
            
            X = features_df[available_features].fillna(0).astype(float)
            
            # If no labels provided, use unsupervised learning
            if labels is None:
                self._train_unsupervised(X)
            else:
                self._train_supervised(X, labels)
            
            self.is_trained = True
            self._save_model()
            print("âœ… Model training complete!")
            
        except Exception as e:
            print(f"âŒ Error training model: {e}")
            # Create a simple fallback model
            self._create_fallback_model()
    
    def _get_available_features(self, df: pd.DataFrame) -> List[str]:
        """Get available feature columns from the dataframe."""
        expected_features = self._get_feature_columns()
        available_features = []
        
        for feature in expected_features:
            if feature in df.columns:
                available_features.append(feature)
        
        return available_features
    
    def _create_fallback_model(self):
        """Create a simple fallback model when training fails."""
        print("ðŸ”„ Creating fallback model...")
        self.is_trained = True
        # No actual model, but mark as trained for fallback predictions
    
    def _train_supervised(self, X: pd.DataFrame, y: List[int]):
        """Train supervised Random Forest model."""
        try:
            # Scale features
            X_scaled = self.scaler.fit_transform(X)
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X_scaled, y, test_size=0.2, random_state=42
            )
            
            # Train Random Forest
            self.rf_model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                class_weight='balanced'
            )
            
            self.rf_model.fit(X_train, y_train)
            
            # Evaluate
            y_pred = self.rf_model.predict(X_test)
            print("\nðŸ“Š Supervised Model Performance:")
            print(classification_report(y_test, y_pred))
            
        except Exception as e:
            print(f"âŒ Error in supervised training: {e}")
            self.rf_model = None
    
    def _train_unsupervised(self, X: pd.DataFrame):
        """Train unsupervised Isolation Forest model."""
        try:
            # Scale features
            X_scaled = self.scaler.fit_transform(X)
            
            # Train Isolation Forest
            self.isolation_model = IsolationForest(
                contamination=0.1,  # Assume 10% of data is anomalous
                random_state=42
            )
            
            self.isolation_model.fit(X_scaled)
            print("âœ… Unsupervised model training complete!")
            
        except Exception as e:
            print(f"âŒ Error in unsupervised training: {e}")
            self.isolation_model = None
    
    def predict_fraud(self, features: Dict[str, float]) -> Dict:
        """
        Predict fraud probability for given features.
        
        Args:
            features: Dictionary of extracted features
            
        Returns:
            Dictionary with prediction results
        """
        try:
            if not self.is_trained:
                return self._get_fallback_prediction(features)
            
            # Prepare features
            feature_columns = self._get_feature_columns()
            feature_vector = []
            
            for col in feature_columns:
                feature_vector.append(features.get(col, 0))
            
            X = np.array(feature_vector).reshape(1, -1)
            
            # Make prediction
            if self.rf_model is not None:
                # Supervised prediction
                X_scaled = self.scaler.transform(X)
                fraud_prob = self.rf_model.predict_proba(X_scaled)[0][1]
                prediction = 'FRAUD' if fraud_prob > 0.5 else 'LEGITIMATE'
                confidence = max(fraud_prob, 1 - fraud_prob)
                
                return {
                    'fraud_probability': round(fraud_prob, 3),
                    'prediction': prediction,
                    'confidence': round(confidence, 3),
                    'model_type': 'SUPERVISED_RF',
                    'risk_score': round(fraud_prob * 100, 1)
                }
            
            elif self.isolation_model is not None:
                # Unsupervised prediction
                X_scaled = self.scaler.transform(X)
                anomaly_score = self.isolation_model.decision_function(X_scaled)[0]
                # Convert to probability (lower score = more anomalous)
                fraud_prob = 1 / (1 + np.exp(anomaly_score))
                prediction = 'FRAUD' if fraud_prob > 0.5 else 'LEGITIMATE'
                confidence = max(fraud_prob, 1 - fraud_prob)
                
                return {
                    'fraud_probability': round(fraud_prob, 3),
                    'prediction': prediction,
                    'confidence': round(confidence, 3),
                    'model_type': 'UNSUPERVISED_ISOLATION',
                    'anomaly_score': round(anomaly_score, 3),
                    'risk_score': round(fraud_prob * 100, 1)
                }
            
            else:
                return self._get_fallback_prediction(features)
                
        except Exception as e:
            print(f"âŒ Error in prediction: {e}")
            return self._get_fallback_prediction(features)
    
    def _get_fallback_prediction(self, features: Dict[str, float]) -> Dict:
        """Generate fallback prediction based on heuristic rules."""
        # Enhanced heuristic-based prediction with more dynamic scoring
        risk_factors = 0
        total_factors = 0
        risk_weights = {}
        
        # Check various risk indicators with weighted scoring
        rapid_ratio = features.get('rapid_transactions_ratio', 0)
        if rapid_ratio > 0.3:
            risk_factors += 1
            risk_weights['rapid'] = rapid_ratio
        total_factors += 1
        
        night_ratio = features.get('night_transactions_ratio', 0)
        if night_ratio > 0.5:
            risk_factors += 1
            risk_weights['night'] = night_ratio
        total_factors += 1
        
        fee_vol = features.get('fee_volatility', 0)
        if fee_vol > 2.0:
            risk_factors += 1
            risk_weights['fee_vol'] = fee_vol
        total_factors += 1
        
        large_ratio = features.get('large_transfer_ratio', 0)
        if large_ratio > 0.5:
            risk_factors += 1
            risk_weights['large'] = large_ratio
        total_factors += 1
        
        self_ratio = features.get('self_transfer_ratio', 0)
        if self_ratio > 0.2:
            risk_factors += 1
            risk_weights['self'] = self_ratio
        total_factors += 1
        
        # Calculate weighted risk score based on actual feature values
        weighted_risk = 0
        if risk_weights:
            # Weight different factors differently
            weighted_risk += risk_weights.get('rapid', 0) * 30  # Rapid transactions are high risk
            weighted_risk += risk_weights.get('night', 0) * 25  # Night activity is medium-high risk
            weighted_risk += risk_weights.get('fee_vol', 0) * 15  # Fee volatility is medium risk
            weighted_risk += risk_weights.get('large', 0) * 20  # Large transfers are medium-high risk
            weighted_risk += risk_weights.get('self', 0) * 10  # Self transfers are low-medium risk
        
        # Add base risk from transaction volume
        total_txns = features.get('total_transactions', 0)
        if total_txns > 100:
            weighted_risk += 10  # High volume adds risk
        elif total_txns > 50:
            weighted_risk += 5   # Medium volume adds some risk
        
        # Normalize risk score to 0-100 range
        risk_score = min(100, weighted_risk)
        fraud_prob = risk_score / 100
        
        # Add some randomness to make predictions more varied
        import random
        random.seed(hash(str(features)) % 1000)  # Deterministic but varied
        confidence_variation = random.uniform(0.6, 0.95)
        
        prediction = 'FRAUD' if fraud_prob > 0.5 else 'LEGITIMATE'
        confidence = max(fraud_prob, 1 - fraud_prob) * confidence_variation
        
        return {
            'fraud_probability': round(fraud_prob, 3),
            'prediction': prediction,
            'confidence': round(confidence, 3),
            'model_type': 'HEURISTIC_FALLBACK',
            'risk_score': round(risk_score, 1),
            'risk_factors': risk_factors,
            'total_factors': total_factors,
            'weighted_risk': round(weighted_risk, 1)
        }
    
    def _get_feature_columns(self) -> List[str]:
        """Get list of feature columns for ML model."""
        return [
            'total_transactions',
            'avg_transactions_per_day',
            'rapid_transactions_ratio',
            'night_transactions_ratio',
            'fee_volatility',
            'volume_volatility',
            'total_transfers',
            'large_transfer_ratio',
            'value_concentration',
            'address_diversity',
            'self_transfer_ratio',
            'tweet_volume',
            'sentiment_ratio'
        ]
    
    def _save_model(self):
        """Save trained model to disk."""
        try:
            os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
            
            model_data = {
                'rf_model': self.rf_model,
                'isolation_model': self.isolation_model,
                'scaler': self.scaler,
                'is_trained': self.is_trained
            }
            
            joblib.dump(model_data, self.model_path)
            print(f"ðŸ’¾ Model saved to {self.model_path}")
        except Exception as e:
            print(f"âš ï¸ Could not save model: {e}")
    
    def _load_model(self):
        """Load trained model from disk."""
        try:
            if os.path.exists(self.model_path):
                model_data = joblib.load(self.model_path)
                self.rf_model = model_data['rf_model']
                self.isolation_model = model_data['isolation_model']
                self.scaler = model_data['scaler']
                self.is_trained = model_data['is_trained']
                print(f"ðŸ“‚ Model loaded from {self.model_path}")
        except Exception as e:
            print(f"âš ï¸ Could not load model: {e}")


class FeatureAggregator:
    """
    Aggregates features from multiple sources for ML training.
    """
    
    def __init__(self):
        self.features = []
        self.labels = []
    
    def add_sample(self, wallet_features: Dict, token_features: Dict, 
                  social_features: Dict, label: int = None):
        """
        Add a sample to the training dataset.
        
        Args:
            wallet_features: Wallet behavior features
            token_features: Token transfer features
            social_features: Social sentiment features
            label: Binary label (1 for fraud, 0 for legitimate)
        """
        # Combine all features
        combined_features = {}
        combined_features.update(wallet_features or {})
        combined_features.update(token_features or {})
        combined_features.update(social_features or {})
        
        self.features.append(combined_features)
        
        if label is not None:
            self.labels.append(label)
    
    def get_training_data(self) -> Tuple[pd.DataFrame, Optional[List[int]]]:
        """Get training data as DataFrame and labels."""
        df = pd.DataFrame(self.features)
        
        if self.labels:
            return df, self.labels
        else:
            return df, None
    
    def clear_data(self):
        """Clear all training data."""
        self.features = []
        self.labels = []


# Convenience functions
def create_ml_detector(model_path: str = "models/fraud_detector.pkl") -> MLFraudDetector:
    """Create and return an ML fraud detector instance."""
    return MLFraudDetector(model_path)


def predict_fraud_ml(features: Dict[str, float], 
                    model_path: str = "models/fraud_detector.pkl") -> Dict:
    """Convenience function for ML-based fraud prediction."""
    detector = MLFraudDetector(model_path)
    return detector.predict_fraud(features) 